traceData_final$orgUnit <- factor(traceData_final$orgUnit, ordered = FALSE)
traceData_final$Occupation_simple <- factor(traceData_final$Occupation_simple, ordered = FALSE)
traceData_final$gender <- factor(traceData_final$gender, ordered = FALSE)
traceData_final$married_simple <- factor(traceData_final$married_simple, ordered = FALSE)
traceData_final$pregnant_simple <- factor(traceData_final$pregnant_simple, ordered = FALSE)
traceData_final$recency_interpretation <- factor(traceData_final$recency_interpretation, ordered = FALSE)
#####
traceData_ready <- traceData_final %>%
filter(recency_interpretation != "Negative")
traceData_ready <- traceData_final %>%
filter(recency_interpretation != "Negative")
traceData_ready <- traceData_final %>%
filter(recency_interpretation != "Invalid")
recency_col <- grep("^recency|Recency", names(traceData_ready), value = TRUE)
traceData_ready <- traceData_ready %>%
filter(!(.data[[recency_col]] == "Invalid"))
write_xlsx(traceData_ready, "traceData_ready_keeke.xlsx")
traceData_ready$recency_interpretation <- factor(traceData_ready$recency_interpretation, ordered = FALSE)
summary(traceData_ready)
prop.table(table(traceData_ready$recency_interpretation))
traceData_ready$recency_interpretation <- ifelse(
traceData_ready$recency_interpretation == "Recent", 1L, 0L)
write_xlsx(traceData_ready, "traceData_ready_keeke.xlsx")
prop.table(table(traceData_ready$recency_interpretation))
seed <- 123 # set a seed for reproducibility
set.seed(seed)
split <- sample.split(traceData_ready$recency_interpretation, SplitRatio = 0.7)
train_trace <- subset(traceData_ready, split == "TRUE")
test_trace <- subset(traceData_ready, split == "FALSE")
write_xlsx(train_trace, "train_trace.xlsx")
prop.table(table(train_trace$recency_interpretation))
prop.table(table(test_trace$recency_interpretation))
str(test_trace$recency_interpretation)
prop.table(table(test_trace$recency_interpretation))
summary(table(test_trace$recency_interpretation))
summary(test_trace$recency_interpretation)
train_trace$recency_interpretation <- factor(train_trace$recency_interpretation, ordered = FALSE)
test_trace$recency_interpretation <- factor(test_trace$recency_interpretation, ordered = FALSE)
prop.table(table(train_trace$recency_interpretation))
view(table(train_trace$recency_interpretation))
view(table(train_trace$recency_interpretation))
typeof(table(train_trace$recency_interpretation))
train_trace_h2o <- as.h2o(train_trace)
test_trace_h2o <- as.h2o(test_trace)
target <- "recency_interpretation"
predictors <- setdiff(names(train_trace), target)
trace_nb <- h2o.naiveBayes(
x = predictors,
y = target,
training_frame = train_trace_h2o,
laplace = 0, # a smoothing parameter for categories with
nfolds = 5, # for 5-fold CV
seed = seed
)
# check performance of model:
h2o.performance(trace_nb)
preds_nb_train <- h2o.predict(trace_nb, train_trace_h2o)
preds_nb_test <- h2o.predict(trace_nb, test_trace_h2o)
preds_nb_train <- as.data.frame(preds_nb_train)
preds_nb_test <- as.data.frame(preds_nb_test)
view(preds_nb_train)
train_nb_pred <- cbind(train_trace, preds_nb_train[, 3, drop = FALSE]) # drop = FALSE keeps the original name of this 3rd column (p1) in the resulting data frame
test_nb_pred <- cbind(test_trace, preds_nb_test[, 3, drop = FALSE])
threshold <- 0.5
view(train_nb_pred)
view(train_nb_pred$recency_interpretation)
typeof(train_nb_pred$recency_interpretation)
str(train_nb_pred$recency_interpretation)
# training
train_nb_pred$pred_class <- factor(ifelse(
train_nb_pred$p1 > threshold,
1L,
0L
))
view(train_nb_pred)
# predicted classes first then actual classes
confusionMatrix(
train_nb_pred$pred_class,
train_nb_pred$p1,
positive = "1",
mode = "everything"
)
view(train_nb_pred$pred_class)
summary(train_nb_pred$pred_class)
summary(train_nb_pred$p1)
str(train_nb_pred$p1)
levels(train_nb_pred$p1)
levels(train_nb_pred$pred_class)
train_nb_pred$pred_class <- factor(ifelse(
train_nb_pred$p1 > threshold,
"1",
"0"
), levels = c("0", "1"))
train_nb_pred$recency_interpretation <- factor(
train_nb_pred$recency_interpretation,
levels = c("0", "1")
)
confusionMatrix(
data = train_nb_pred$pred_class,        # Predicted Class
reference = train_nb_pred$recency_interpretation, # Actual Class
positive = "1",
mode = "everything"
)
roc_nb_train <- roc(train_nb_pred$recency_interpretation, train_nb_pred$p1)
auc(roc_nb_train)
plot(roc_nb_train, main = "ROC Curve - Training Set")
# Create confusion matrix using a threshold:
threshold <- 0.5
# ---------------------------------------------------------
# 1. TRAINING SET EVALUATION
# ---------------------------------------------------------
# Create predicted class based on threshold
train_nb_pred$pred_class <- factor(ifelse(
train_nb_pred$p1 > threshold,
"1",
"0"
), levels = c("0", "1"))
# Ensure the actual truth is a factor with the same levels
train_nb_pred$recency_interpretation <- factor(
train_nb_pred$recency_interpretation,
levels = c("0", "1")
)
# FIX: Pass 'recency_interpretation' (Truth) instead of 'p1'
confusionMatrix(
data = train_nb_pred$pred_class,        # Predicted Class
reference = train_nb_pred$recency_interpretation, # Actual Class
positive = "1",
mode = "everything"
)
# ROC and AUC (This requires Numeric inputs, so we calculate it separately)
# We use as.numeric(as.character(...)) to safely convert factors back to numbers if needed,
# but p1 is already numeric.
roc_nb_train <- roc(train_nb_pred$recency_interpretation, train_nb_pred$p1)
auc(roc_nb_train)
plot(roc_nb_train, main = "ROC Curve - Training Set")
# ---------------------------------------------------------
# 2. TEST SET EVALUATION
# ---------------------------------------------------------
# Create predicted class based on threshold
test_nb_pred$pred_class <- factor(ifelse(
test_nb_pred$p1 > threshold,
"1",
"0"
), levels = c("0", "1"))
# Ensure the actual truth is a factor with the same levels
test_nb_pred$recency_interpretation <- factor(
test_nb_pred$recency_interpretation,
levels = c("0", "1")
)
# FIX: Pass 'recency_interpretation' instead of 'p1'
# REMOVED: The lines that converted recency_interpretation to as.numeric()
confusionMatrix(
data = test_nb_pred$pred_class,
reference = test_nb_pred$recency_interpretation,
positive = "1",
mode = "everything"
)
# ROC and AUC for Test
roc_nb_test <- roc(test_nb_pred$recency_interpretation, test_nb_pred$p1)
auc(roc_nb_test)
plot(roc_nb_test, main = "ROC Curve - Test Set")
# Fit the logistic regression model
income_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_income_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE # optional: get p-values
)
# Fit the logistic regression model
income_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_trace_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE # optional: get p-values
)
df <- income_LR@model[["coefficients_table"]]
# Fit the logistic regression model
trace_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_trace_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE # optional: get p-values
)
df <- trace_LR@model[["coefficients_table"]]
trace_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_trace_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE # optional: get p-values
)
trace_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_trace_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE # optional: get p-values
remove_collinear_columns = TRUE
# Fit the logistic regression model
trace_LR <- h2o.glm(
x = predictors,
y = target,
training_frame = train_trace_h2o,
family = "binomial", # logistic regression
lambda = 0, # no regularization (like classical GLM)
compute_p_values = TRUE, # optional: get p-values
remove_collinear_columns = TRUE
)
df <- trace_LR@model[["coefficients_table"]]
options(scipen = 999)
df$OR <- exp(df[,2])
df$p_value <- round(df$p_value,4)
df
view(df)
# Save predicted probabilities
preds_LR_train <- h2o.predict(trace_LR, train_trace_h2o)
preds_LR_test <- h2o.predict(trace_LR, test_trace_h2o)
# Convert predictions to R data.frames to extract from H2O environment:
preds_LR_train <- as.data.frame(preds_LR_train)
preds_LR_test <- as.data.frame(preds_LR_test)
train_LR_pred <- cbind(train_trace, preds_LR_train[, 3, drop = FALSE]) # drop = FALSE keeps the original name of this 3rd column (p1) in the resulting data frame
test_LR_pred <- cbind(test_trace, preds_LR_test[, 3, drop = FALSE])
threshold <- 0.5
# training
train_LR_pred$pred_class <- factor(ifelse(
train_LR_pred$p1 > threshold,
"1",
"0"
))
# predicted classes first then actual classes
confusionMatrix(
train_LR_pred$pred_class,
train_LR_pred$income,
positive = "1",
mode = "everything"
)
train_LR_pred$pred_class <- factor(ifelse(
train_LR_pred$p1 > threshold,
"1",
"0"
), levels = c("0", "1"))
confusionMatrix(
data = train_LR_pred$pred_class,        # Predicted Class
reference = train_LR_pred$recency_interpretation, # Actual Class
positive = "1",
mode = "everything"
)
# actual classes first then predicted probabilities
roc_LR_train <- roc(train_LR_pred$income, train_LR_pred$p1)
View(train_LR_pred)
roc_LR_train <- roc(train_LR_pred$pred_class, train_LR_pred$p1)
auc(roc_LR_train)
plot(roc_LR_train)
# test
test_LR_pred$pred_class <- factor(ifelse(test_LR_pred$p1 > threshold, "1", "0"))
confusionMatrix(
data = test_LR_pred$pred_class,        # Predicted Class
reference = test_LR_pred$recency_interpretation, # Actual Class
positive = "1",
mode = "everything"
)
# actual classes first then predicted probabilities
roc_LR_test <- roc(test_LR_pred$income, test_LR_pred$p1)
# actual classes first then predicted probabilities
roc_LR_test <- roc(test_LR_pred$pred_class, test_LR_pred$p1)
auc(roc_LR_test)
plot(roc_LR_test)
?h2o.naiveBayes()
?h2o.glm()
roc_nb_train <- roc(train_nb_pred$recency_interpretation, train_nb_pred$p1)
auc(roc_nb_train)
plot(roc_nb_train, main = "ROC Curve - Training Set | naive Bayes")
roc_nb_test <- roc(test_nb_pred$recency_interpretation, test_nb_pred$p1)
auc(roc_nb_test)
plot(roc_nb_test, main = "ROC Curve - Test Set | naive Bayes")
roc_LR_train <- roc(train_LR_pred$pred_class, train_LR_pred$p1)
auc(roc_LR_train)
plot(roc_LR_train, main = "ROC Curve - Training Set | Logistic Regression")
roc_LR_test <- roc(test_LR_pred$pred_class, test_LR_pred$p1)
auc(roc_LR_test)
plot(roc_LR_test, main = "ROC Curve - Test Set | Logistic Regression")
# Plot (see https://r-charts.com/colors/ for more colours)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for NB, DT and LR"
)
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Plot (see https://r-charts.com/colors/ for more colours)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for NB, DT and LR"
)
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
library(rpart)
library(rpart.plot) # Optional, for plotting the tree
library(pROC)
# Plot (see https://r-charts.com/colors/ for more colours)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for NB, DT and LR"
)
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
library(rpart)
library(rpart.plot)
library(pROC)
preds_lr_h2o <- h2o.predict(trace_LR, test_trace_h2o)
preds_lr_df <- as.data.frame(preds_lr_h2o)
roc_LR_test <- roc(
response = test_trace$recency_interpretation,
predictor = preds_lr_df$p1,
levels = c("0", "1")
)
# Plot (see https://r-charts.com/colors/ for more colours)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison: NB vs DT vs LR"
)
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Plot Base (Naive Bayes)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison: NB vs DT vs LR"
)
# Add Decision Tree (Red)
# Ensure 'roc_DT_test_rpart' exists from the previous step
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# 1. Load necessary libraries
library(rpart)
library(pROC)
# 2. Train the Decision Tree (rpart)
# We use the standard R dataframes (train_trace), NOT the h2o ones
dt_model <- rpart(
recency_interpretation ~ .,
data = train_trace,
method = "class"
)
# 3. Predict probabilities on the Test Set
dt_preds <- predict(dt_model, test_trace, type = "prob")
# 4. Create the missing 'roc_DT_test_rpart' object
# We grab column "1" to get the probability of the positive class
roc_DT_test_rpart <- roc(
response = test_trace$recency_interpretation,
predictor = dt_preds[, "1"],
levels = c("0", "1")
)
# -------------------------------------------------------
# 5. NOW RUN THE PLOT
# -------------------------------------------------------
# Plot Base (Naive Bayes)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for NB, DT and LR"
)
# Add Decision Tree (Red) -> This will work now!
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Add Logistic Regression (Blue)
# (Assuming roc_LR_test exists from the previous step)
lines(roc_LR_test, col = "#009ACD", lwd = 2)
# Add Legend
legend(
"bottomright",
legend = c("Naive Bayes", "Decision tree", "Logistic regression"),
col = c("#458B74", "#CD3333", "#009ACD"),
lwd = 2
)
#################### Combine ROC curves of test set for all models ############
# 1. Load necessary libraries
library(rpart)
library(pROC)
# 2. Train the Decision Tree (rpart)
# We use the standard R dataframes (train_trace), NOT the h2o ones
dt_model <- rpart(
recency_interpretation ~ .,
data = train_trace,
method = "class"
)
# 3. Predict probabilities on the Test Set
dt_preds <- predict(dt_model, test_trace, type = "prob")
# 4. Create the missing 'roc_DT_test_rpart' object
# We grab column "1" to get the probability of the positive class
roc_DT_test_rpart <- roc(
response = test_trace$recency_interpretation,
predictor = dt_preds[, "1"],
levels = c("0", "1")
)
# -------------------------------------------------------
# 5. NOW RUN THE PLOT
# -------------------------------------------------------
# Plot Base (Naive Bayes)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for NB, DT and LR"
)
# Add Decision Tree (Red) -> This will work now!
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Add Logistic Regression (Blue)
# (Assuming roc_LR_test exists from the previous step)
lines(roc_LR_test, col = "#009ACD", lwd = 2)
# Add Legend
legend(
"bottomright",
legend = c("Naive Bayes", "Decision tree", "Logistic regression"),
col = c("#458B74", "#CD3333", "#009ACD"),
lwd = 2
)
#################### Combine ROC curves of test set for all models ############
# 1. Load necessary libraries
library(rpart)
library(pROC)
# 2. Train the Decision Tree (rpart)
# We use the standard R dataframes (train_trace), NOT the h2o ones
dt_model <- rpart(
recency_interpretation ~ .,
data = train_trace,
method = "class"
)
# 3. Predict probabilities on the Test Set
dt_preds <- predict(dt_model, test_trace, type = "prob")
# 4. Create the missing 'roc_DT_test_rpart' object
# We grab column "1" to get the probability of the positive class
roc_DT_test_rpart <- roc(
response = test_trace$recency_interpretation,
predictor = dt_preds[, "1"],
levels = c("0", "1")
)
# -------------------------------------------------------
# 5. NOW RUN THE PLOT
# -------------------------------------------------------
# Plot Base (Naive Bayes)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "ROC Curve Comparison of test set for naive Bayes, Decision Tree and Logistic Regression"
)
# Add Decision Tree (Red) -> This will work now!
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Add Logistic Regression (Blue)
# (Assuming roc_LR_test exists from the previous step)
lines(roc_LR_test, col = "#009ACD", lwd = 2)
# Add Legend
legend(
"bottomright",
legend = c("Naive Bayes", "Decision tree", "Logistic regression"),
col = c("#458B74", "#CD3333", "#009ACD"),
lwd = 2
)
# 1. Load necessary libraries
library(rpart)
library(pROC)
# 2. Train the Decision Tree (rpart)
# We use the standard R dataframes (train_trace), NOT the h2o ones
dt_model <- rpart(
recency_interpretation ~ .,
data = train_trace,
method = "class"
)
# 3. Predict probabilities on the Test Set
dt_preds <- predict(dt_model, test_trace, type = "prob")
# 4. Create the missing 'roc_DT_test_rpart' object
# We grab column "1" to get the probability of the positive class
roc_DT_test_rpart <- roc(
response = test_trace$recency_interpretation,
predictor = dt_preds[, "1"],
levels = c("0", "1")
)
# -------------------------------------------------------
# 5. NOW RUN THE PLOT
# -------------------------------------------------------
# Plot Base (Naive Bayes)
plot(
roc_nb_test,
col = "#458B74",
lwd = 2,
main = "naive Bayes vs Decision Tree vs Logistic Regression"
)
# Add Decision Tree (Red) -> This will work now!
lines(roc_DT_test_rpart, col = "#CD3333", lwd = 2)
# Add Logistic Regression (Blue)
# (Assuming roc_LR_test exists from the previous step)
lines(roc_LR_test, col = "#009ACD", lwd = 2)
# Add Legend
legend(
"bottomright",
legend = c("Naive Bayes", "Decision tree", "Logistic regression"),
col = c("#458B74", "#CD3333", "#009ACD"),
lwd = 2
)
h2o.shutdown(prompt = FALSE)
